\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}

\usepackage[colorlinks=true]{hyperref}
\usepackage[linesnumbered,algoruled,boxed,lined]{algorithm2e}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{caption}
\usepackage{float}
\usepackage{multirow}
\usepackage{placeins}
\usepackage{hhline}
\usepackage{MnSymbol}
\usepackage[scientific-notation=true]{siunitx}
\usepackage{booktabs}

\def\d{\,{\rm d}}               % differential
\def\vc#1{\mathbf{\boldsymbol{#1}}}     % vector
\def\tn#1{\boldsymbol{#1}}
\def\eps{\varepsilon}
\def \E{{\mathsf E}}
\def \D{{{\rm I\kern-.3em D}}}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\def\prtl{\partial}
\def\todo#1{{\color{red}TODO: #1}}
\def\R{\mathbf{R}}
\def\pluseq{\mathrel{+}=}
\def\argdot{{\hspace{0.18em}\cdot\hspace{0.18em}}}
\def\Bavg#1{\Left\langle#1\Right\rangle}
\def\avg#1{\langle#1\rangle}
\def\var#1{\llangle#1\rrangle}
\def\Var{\mathop{\rm Var}}
\def\Cov{\mathop{\rm Cov}}
%\def\log{\mathop{\rm log}}
\def\abs#1{|#1|}
\DeclareMathOperator{\Span}{span}

\def\tvl{\vc{\tilde\lambda}}
\def\vl{{\vc\lambda}}
\def\estvl{{\vc{\hat\lambda}}}
\def\estrho{\hat\rho}
\def\vmu{\vc\mu}
\def\estvmu{{\vc{\hat\mu}}}
\def\vphi{\vc\phi}

\newlength\lengtha \setlength\lengtha{2mm} % table spacing
\newlength\lengthb \setlength\lengthb{0.5mm} % table spacing

\title{Robust Density Estimation Using Maximal Entropy Multilevel Monte Carlo method. }
\author{jan.brezina }
\date{September 2018}


\begin{document}

\maketitle

\section{Introduction}
Long term safety of an underground radioactive waste repository is assured primarily by the dilution and 
prolonged transport of the contaminants through the suitable rock formations. Maximal concentrations of the contaminant on the surface is of major concern. Unfortunately, reliable prediction of the surface concentration is not tractable due to lack of data about highly heterogeneous rock environment. In order to deal with these uncertainties we can model unknown rock properties as random fields and predict probability density function of the surface density. 
The classical Monte Carlo method needs at least several thousands of samples, which is prohibitively costly in the case of complex simulations. The Multilevel Monte Carlo (MLMC) method introduced by Giles \cite{Giles2008}, \cite{Giles2015} provides a way to estimate mean of an observable at the cost comparable to few forward simulations. Basic idea is to do a high volume of samples using a less accurate but cheap approximation and much fewer samples of the difference between the observable and its approximation. If the difference has much smaller variance we obtain the mean estimate with the same accuracy at a fraction of cost.  The MLMC method has been already studied and analyzed in connection with a variety of PDE problems: elliptic equations \cite{Barth2011a}, \cite{Cliffe2011a}, \cite{Abdulle2013}, parabolic equations \cite{Barth2013},
multiphase flow \cite{Muller2013}, \cite{Lu2016} and others.


Maximal entropy:
\cite{Barron1991}, \cite{Bierig2016a}

Elliptic eq. with log normal coefficients \cite{Graham2015}






 However, the limitation is that MLMC can compute only approximation of expectation of any random variable. Therefore it is straight forward to make an approximation of the mean, the variance and other moments, while it is not possible to approximate the quantiles and the distribution function directly. On the other way, the moment function or characteristic function are given in terms of expectations, which motivates a general approximation of the density function in terms of generalized moments.

Possible outcomes:
\begin{itemize}
    \item Research for suitable statistical models for complex hydrology problems (porosity, conductivity, dispersion, fracture opening, fracture density, ... dependency on the length scale).
    \item Demonstrate efficiency of MLMC for estimation various moments of relevant quantities of interest.
    \item Demonstrate efficient generation of samples for various types of correlations.
    \item Demonstrate reconstruction of the density function.
\end{itemize}

\section{Multilevel Monte Carlo method}
In this section, we shall introduce the MLMC estimator and consider its application to a random variable based on a solution to a PDE.
Let $(\Omega, \Sigma, P)$ be a probability space and $X(\omega)$ be a random variable
that can not be sampled exactly, but we are able to sample its approximation $X^h(\omega)$. 
Consider classical MC approximation of the expectation $\E X$ by the sample mean:
\[
    \avg{X}_N = \avg{X^h}_N = \frac{1}{N}\sum_{i=1}^{N} X^h(\omega_i).
\]
As the estimator is unbiased, the mean square error (MSE) can be decomposed as:
\begin{equation}
    \label{eq:mc_mse}
    \E[\avg{X}_N - \E X]^2 = \E[\avg{X}_N - \E X^h ]^2 + \abs{\E X^h - \E X}^2
\end{equation}
The second term is error of the approximation $X^h$, which we assume to be sufficiently small further on.
The first term on the right hand side is the sampling error or the variance of the estimator:
\[
    \Var(\avg{X}_N) = \E[\avg{X}_N - \E X^h ]^2 = \frac{1}{N}\Var(X^h).
\]
 Classical MC method diminishes the sampling error just by increasing $N$. The idea of the MLMC method is to introduce another approximation $X^H$ that is cheaper to sample and use the composed estimator
\[
	\E X^h \approx \avg{X}_{M,N} = \avg{X^H}_M	 + \avg{X^h - X^H}_N. 
\]
the estimator is unbiased with variance
\[
	\Var(\avg{X}_{M,N}) =  \frac{1}{M}\Var{X^H} + \frac{1}{N}\Var[X^h-X^H]. 
\]
Now we can increase the precision of the estimator by increasing the number $M$ of the cheap coarse samples while keeping the number $N$ of the expensive fine samples low provided the variance
$\Var[X^h-X^H]$ is small. 

This aproach can be futher extened using a sequence of approximations $X^l = X^{h_l}$, for $l=1,\dots, L$.
Setting $X^0 = 0$ for the sake of consistency,
we obtain the MLMC estimator of $\E X$ in the form
\begin{equation}
    \label{eq:mlmc_est}
	\avg{X^L}_{\vc N} = \sum_{l=1}^L \avg{X^l - X^{l-1}}_{N_l} = \sum_{l=1}^L \avg{\Delta^l X}_{N_l}
\end{equation}
where $\vc N=(n_1,\dots, n_L)$ is the \emph{sampling vector} of the number of samples on individual levels 
and by $\Delta^l X(\omega) = X^l(\omega) - X^{l-1}(\omega)$ we denote level differences. Note that 
individual samples of the level differences must be based on the same realization $\omega$. 
Variance of the MLMC estimator is:
\begin{equation}
    \label{eq:var_mlmc}
    \Var( \avg{X^L}_{\vc N}) = \sum_{l=1}^L \frac{V_l}{N_l},\quad V_l = \Var(\avg{\Delta^l X}_{N_l}).
\end{equation}
The level variances $V_l$ can be estimated using the standard unbiased estimator:
\[
  V_l \approx \widehat{V}_l  = \frac{1}{N_l-1} \sum_{i=1}^{N_l} \big(\Delta^l X_i - \avg{\Delta^l X}_{N_l}\big)^2
\]


\subsection{Aplication to finite element solutions}
\todo{Very preliminary, make it more precise. Refer to other applied papers.}
In particular we are interested in the case $X=F(u)$, where $F$ is a functional representing an observation of
the solution $u$ to a PDE with some random data $d(\omega)$. The approximation $X^l = F(u^{h_l})$, $l=0,\dots,L$ \todo{Shouldn't $l$ be from $1$? } is the same observation performed for an approximate solution $u^{h_l}$ computed on a mesh with the maximal element size $h_l$. Further on we assume $h_l = 2^{-\alpha l}$. Let $u$ be from a suitable space 
$W^* \subset W$, $u_h$ from $W_{h_l} \subset W$ and assume the usual error estimate in the form:
\[
	\norm{u - u_h}_{W} \le h^s \norm{u}_{W^*}
\] 
for some $s\ge 1$. See e.g. \cite{Evans1998}.
If $F:W \to R$ is bounded, then for $h=h_l$, $H=h_{l-1}$, we have
\[
	\abs{X^l - X^{l-1}} = \abs{F(u^h) - F(u^H)} \le \norm{F}\norm{u^h - u^H}_W \le C H^s\norm{u}_{W^*}
\]
and therefore 
\begin{equation}
    \label{eq:level_var_pde_est}
    V_l =  \E[X^l - X^{l-1}]^2 = C_u 2^{-\beta l},\quad \beta = 2s\alpha.
\end{equation}
By the same token we have 
\begin{equation}
    \label{eq:aprox_pde_est}
    \abs{\E X - \E X^L}^2 = C_u 2^{-2s\alpha L}
\end{equation}
Evaluation cost $C_l$ of the single sample difference $X^{l-1}(\omega) - X^l(\omega)$ is
\[
  C_l = \mathcal O( h_l^{-p}) \le C 2^{\gamma l}, \quad \gamma = p\alpha.
\]
For example in the case of linear elliptic equations using a multigrid solver one can get $p$ close to the problem dimension $d=1,2,3$. For more complex or time-dependent problems the power $p$ can be larger, but 
still it is usually independent of the choice of order of approximation $s$. Therefore, we may assume
$\beta > \gamma$. Applying \cite[Theorem 1]{Giles2015} for this case we conclude that for any target error 
$\epsilon< \frac{1}{e}$ we can found the sample vector $\vc N$ such that the MLMC estimator have the target error
\[
  \E[\avg{X}_N - \E X]^2 \le \epsilon^2
\]
and the total cost bounded as $C < c \epsilon^{-2}$.


\subsection{Optimal choice of number of samples}
Efficiency of the MLMC estimator \eqref{eq:mlmc_est} depends on the optimal choice of the sample vector $\vc N$. Basic analysis was done by Giles in \cite{Giles2008} and \cite{Giles2015}. In this section we report the formulas \eqref{eq:opt_n_for_var} \eqref{eq:opt_n_for_cost} for optimal $\vc N$ in the case of given target variance $V$ or in the case of target cost $C$ respectively. Further, we propose a slight modification suitable for practical usage.

Let  the level variances $V_l$ be given and let us denote $C_l$ the computational cost of a single sample of the difference $\Delta X^l$ at the level $l=1,\dots, L$. For a given target variance $V$ (c.f. \eqref{eq:var_mlmc}), we want to minimize the total cost
\begin{equation}
    \label{eq:total_cost}
	C = \sum_{l=1}^{L} C_l N_l.
\end{equation}
This leads to a minimization problem for the functional
\[
	\Phi(N_1, \dots, N_L,\lambda) = \sum_l C_l N_l + \lambda \Big(\sum_l \frac{V_l}{N_l} - V\Big).
\]
Straightforward calculation then provides optimal choice of $\vc N$ for given $V$:
\begin{equation}
	\label{eq:opt_n_for_var}
	N_l^V = \sqrt{\frac{V_l}{C_l}} \frac{S}{V},\ \text{for }l=1,\dots, L,\quad \text{with } S = \sum_{i=1}^L \sqrt{V_i C_i}.
\end{equation}
Similarly, we obtain $\vc N$ that minimize the total variance for the fixed total cost $C$ as
\begin{equation}
	\label{eq:opt_n_for_cost}
	N_l^C = \sqrt{\frac{V_l}{C_l}} \frac{C}{S}
	, \quad \text{for }l=1,\dots, L.
\end{equation}
Considering $V_l \le C_{v} 2^{-\beta l}$ and $C_l \le C_{c} 2^{\gamma l}$ we have total cost
\[
   C \le \frac{S^2}{V} = C_v C_c c \epsilon^{-2},
\]
where 
\[ 
   c = \left(\sum_{l=1}^{L} q^l\right)^2 \le (1-q)^{-2},\ q=2^\frac{\gamma - \beta}{2}
\]
for given target variance $V=\epsilon^2$. Similarly, $V \le S^2 / C$ for given target cost $C$.



Formulas \eqref{eq:opt_n_for_var} and \eqref{eq:opt_n_for_cost} are based on the knowledge of the level variances $V_l$ and computational costs $C_l$ which are usually not known and must be estimated (see Section \ref{sec:VarEst}). These small errors lead to large changes in the optimal sample vector since related minimized functionals are quite flat at the optimal point.  This often results in overestimating the number of samples, especially on lower levels. To mitigate this problem we rather use
\[
	N_l^{V*} = \min( N_l^V, \frac{V_l L}{V} )
\]
for prescribed total variance $V$ and
\[
	N_l^{C*} = \min( N_l^C, \frac{C}{C_l L} )
\]
in the case of fixed cost $C$. This modification increase the total variance or total cost at most by $V$ and $C$ respectively, while keeping  $N_l$ reasonable. Indeed, let
\[
	N_l^{V*} = \frac{V_l L}{V} < N_l^V 
\]
happen for a single level $l$, then an increase in the total variance is
\[
    V^* - V = \frac{V}{L} - \frac{V_l}{N_l^V} \le \frac{V}{L}
\]
and similarly for $N_l^{C*}$.



\subsection{Estimate level variances and costs}\label{est_var_cost}
\label{sec:VarEst}
As the exact values for $V_l$ and $C_l$ in \eqref{eq:opt_n_for_var} and \eqref{eq:opt_n_for_cost} are not available they are estimated from initial fraction of the samples. In particular $V_l$ is estimated by standard unbiased variance estimater $\widehat V_l$ and $C_l$ by a mean of times of collected samples 
$\widehat C_l$. Error of these estimates can be quite large especially on higher levels. 



In order to make MLMC algorithm fully automatic, we have to estimate level variances $V_l$ from already collected samples. Since direct estimates are too inaccurate for higher levels we rather set up a regression model for $V_l$ and estimate its parameters using data from all levels. Moreover, we also combine data for different moments.


Using the approximation:
\[
 \log \widehat V_l = \log V_l + \epsilon + O(\epsilon^2), \quad \epsilon = \frac{\widehat V_l - V_l}{V_l}
\]
we obtain an approximation for its variance as:
\[
   \Var(\log \widehat V_l) = \Var\left[ \frac{\sum_{i=1}^{N_l} Y_i^2}{N_l -1}\right]  \approx \frac{1}{N-1}
\]
for some standardized i.i.d. samples $Y_i$.

Under the assumption, that regression model $lm(\vc \alpha)$ fits well we have 
\[
 1 = (N-1) \Var[log \widehat V] = \E[ (\log \widehat V - lm(\alpha))^2 (N-1) ] 
\]


\[
  W_l = \sqrt{N_l-1}*(\log \widehat V_l - lm(\widehat{\vc\alpha}))
\]  
  is approximately $N(0,1)$. We use test for variance and possibly 
reduce the number of levels used for regression. If
\[
  \sum_l W_l^2 > \chi^2_{N-1}(1-\alpha). 
\]
we reject hypothesis of the good fit.
\todo{Move variance approximation to notes since we need more testing}
Remarks:
\begin{itemize}
 \item Empirically, for the Flow problem, 9 level, for quadratic model and using levels 3,4,5,6,7,8 for the fit we obtain variances that are significantly better then raw variances.
 \item The hypothesis testing doesn't work. Even if it holds that $W_l$ is approximately $N(0,1)$.
 We obtain much higher values for the statistics. Further it depends on number of moments and number of used levels. 
 \item It behaves like it have much greater variance then 1, since $W_l^2$ values corresponds always to the p-values close to 1, which corresponds to big error of the fit.
 \item Questionable how it may work for smaller number of levels.
 \item Must also be tested for different simulations.
\end{itemize}




Let us assume that the level difference $Z_l^r = X_l^r - X^r_{l-l}$ for level $l=1,\dots L$ and a moment $r$ is normally distributed 
with mean estimate $\hat{Z}_l^r$ and variance estimate $V_l^r$ is
$$
W_l^r = \sum_{i} (Z^r_{l,i} - \hat{Z}_l^r)^2 / (n_l - 1).
$$

Where $Y_l = (n_l -1)W_l^r/V_l^r$ have distribution $\chi^2_{n_l -1}$.

We assume a simple model:
$$
  U_l^r = \log W_l^r =  \alpha_r + \beta \log h_l + \gamma (\log h_l)^2.
$$
where $h_l$ is a mesh step on level $l$ and $\alpha_r$, $\beta$ are parameters. 
For the variance of $U_l^r$ we have:
$$
  \D U_l^r = \D \log \Big( \frac{Y_l V_l^r}{n_l - 1} \Big) = \D \log (Y_l/N_l) = \epsilon_l
$$
with $N_l = n_l - 1$.

Then we estimate model parameters $\alpha_r$, $\beta$ from the least squares problem:
$$
    \tn{X}^T \tn{D}^2 \tn{X} \vc{b} = \tn{X}^T \tn{D} \vc{U},
    \ D_{i,i} = 1/\epsilon_{l}.
$$

Values of $\epsilon_l$ as a function of $l$ will be determined by MC. Note also that using the \href{https://en.wikipedia.org/wiki/Probability_density_function#Dependent_variables_and_change_of_variables}{Jacobi transform} we obtain PDF of $\epsilon_l$ as:
$$
    f(\epsilon) = \exp(\epsilon)N_l\chi^2_{n-1}(\exp(\epsilon)N_l).
$$
For small values of $N_l$ we compute $\D \epsilon_l$ using numerical quadrature. For sufficiently large $N_l$ we approximate $\chi^2$ by normal distribution:
$$
    f(\epsilon) = \exp(\epsilon)N_l\chi^2_{n-1}(\exp(\epsilon)N_l).
$$

\begin{figure}
 \includegraphics[width=0.55\textwidth]{bs_var_var_20.pdf}
 \hspace{-0.1\textwidth}
 \includegraphics[width=0.55\textwidth]{bs_var_var_Nl.pdf}
 \caption{Estimate of error of estimators for $V^r$, $V_l^r$. Left figure demonstrate independece on level, right figure independence on $N_l$. Bootstrapping use 300 samples.}
 \label{fig:bs_var_var}
\end{figure}

\begin{figure}
 \includegraphics[width=0.55\textwidth]{bs_var_var_composition.pdf}
 \caption{BS estimate of the MSE of the variance estimate and its dropdown to individual levels. Realistic sample vector for a target variance $1e-4$.}
 \label{fig:bs_var_var_comp}
\end{figure}


\subsubsection{Uvahy}
Cilem je ziskat odhad rozptylu MLMC estimatoru, tak aby skutecny rozptyl byl mensi s danou pravdepodobnosti. 

Notes:
\begin{itemize}
    \item Variance $W_l$ of the level variance estimator is:
    \[
        Var \hat\sigma_l^2 \approx \hat\sigma_l^2 \frac{2}{N_l - 1}
    \]
    \dots assuming
    \[
    Z_l = \sum_{i=1}^{n_l}\Big(\frac{\Delta X^l_i}{\sigma_l}\Big)^2
    \]
    be approximately $\chi^2_k$ with $k=n_l$. This corresponds to the assumption that level differences $\Delta X_l$ have normal distribution. 
    \item At least for flow test case it seems that higher moments have smaller $W_l$ then first few (up to 10) moments. The differences in $W_l$ for individual moments are about one order of magnitude.  This effect is more pronounced for levels with many samples so it seems to be real. Differences between levels are also about one order of magnitude.
    \item These observations are not in contradiction with approximation of variance of  $\log W_l$.
\end{itemize}


\section{Maximal Entropy method}\label{mem}

The MLMC estimator \eqref{eq:mlmc_est} can be applied to estimate any generalized moment $\E[\phi(X)]$ 
of the random variable $X$ using the same set of random samples. Taking a suitable 
set of such moments it is possible to construct an approximation to the PDF of $X$. This section first
summarizes the Maximal Entropy method and related theoretical results slightly simplifying proofs from \cite{Barron1991}. Later we provide a natural selection of the polynomial basis based on the estimate of the covariance matrix. We show that this choice significantly improves the convergence of the related optimization problem.

Let $X$ be a continuous random variable with density function $\rho$ that have bounded support
$supp(\rho) \subset \Omega$ for a finite interval $\Omega \subset \R$. Further consider generalized moments
\begin{equation}
    \label{eq:gen_moments}
    \mu_m = \E_{\rho}[\phi_m(X)], \quad 
\end{equation}
for a finite set of linearly independent moment functions $\phi_1=1$, $\phi_m\in C^r(\Omega)$, $m=2,\dots, M$ forming a base of the space
\[
    \mathcal V_M = \Span\{\phi_m, m=1,\dots, M\}.
\] 
We shall use also the vector notation $\vphi = (\phi_1, \dots,\phi_M)$. The vector of moment values
$\vmu$ must be from $\mathcal M \subset \R^M$ with $\mu_1 = 1$.

Applying the MLMC estimator \eqref{eq:mlmc_est} and estimate of the variance \eqref{eq:var_mlmc} 
to the random variables $\phi_m(X)$, $m=1,\dots, M$ we obtain estimates of generalized moments
\begin{equation}
    \label{eq:mlmc_est_moments}
    \estvmu = \avg{\vphi} = \sum_{l=1}^L \frac{1}{N_l} \sum_{i=1}^{N_l} \Delta^l\vphi(X_i^l)
\end{equation}
as well as an estimate for the error:
\[
    \Var(\avg{\vphi}_{\vc N}) =  \sum_{l=1}^{L} \frac{1}{N_l} \Var{\Delta^l\vphi} 
    \approx \sum_{l=1}^{L} \frac{1}{N_l} \var{\Delta^l\vphi}_{N_l}.
\]

Similarly, we can obtain MLMC estimate of the covariance matrix $\tn C = \Cov{\vc\phi}$: 
\begin{equation}
    \label{eq:mlmc_est_cov}
    \avg{\tn C} = \avg{(\vphi - \estvmu)\otimes(\vphi - \estvmu)}_{\vc N} = 
    \sum_{l=1}^L \frac{1}{N_l} \sum_{i=1}^{N_l} \Delta^l\Big[ 
    \big(\vphi(X_i^l) - \estvmu\big)\otimes\big(\vphi(X_i^l) - \estvmu\big)\Big].
\end{equation}
The same set of samples can be used in estimators \eqref{eq:mlmc_est}, \eqref{eq:mlmc_est_moments}, and \eqref{eq:mlmc_est_cov} as well as in the estimators of their errors.


Our next goal is to find an approximation $\rho^M$ to the unknown density $\rho$ based only on values $\vmu \in \mathcal M$ of the generalized moments. From the infinite set of such densities, we choose the one that maximizes Shannon's entropy,
$S = -\E_\rho[\log \rho]$,
which corresponds to the minimization of any additional information. 
In a slight generalization of this approach, we assume a prior density $p$ and going to minimize the Kullback-Leibler divergence:
\[
    D(\rho\Vert p) = \int_\Omega \rho \log(\rho / p) \d x,
\]
under constrain $\E_\rho[ \vphi ] = \vmu$. 
% This leads to minimization of the functional:
% \begin{align}
%     \Phi(\rho, \vc\lambda) 
%     &= 
%     \int_\Omega \rho \Big( \vc\lambda \cdot \vc \phi - \log(\rho / p) \Big) \d x - \vc\lambda \cdot \vc\mu.
% \end{align}
Applying the Lagrange multipliers and taking variation with respect to $\rho$ the optimality conditions lead to the density in the form (see also \cite{...}):
\[
    \rho_{\vl}(x) = p e^{\vl\cdot\vphi}
\]
where we assume $\phi_1(x) = 1$ and $\mu_1 =1$. The parameters $\vl$ are solution
to the nonlinear system:
\begin{equation}
    \label{eq:moment_system}
    \vc G(\vl) = \int_\Omega \vphi\, p e^{\vl\cdot \vphi}\d x - \vmu = 0
\end{equation}
with the Jacobi matrix:
\[
    \tn H = \int_\Omega \vphi \otimes \vphi\, 
    p e^{\vl\cdot\vphi} \d x.
\]
\begin{lemma}
If $p$ is strictly positive and measurable on $\Omega$ and functions $\phi_m$,  $m=1,\dots,M$ are linearly independent, then matrix $\tn H$ is symmetric positive definite for any $\vl$.
\end{lemma}
\begin{proof}
For any non-zero $\vc u \in \R^M$ the function $f_{\vc u}(x) = \sum_m u_m \phi_m(x)$ is nonzero and thus
\[
    \vc u^T \tn H \vc u = \int_{\Omega} f_{\vc u}^2 p \exp\Big\{ \vc\lambda\cdot \vc\phi\Big\} \d x > 0.
\]
\end{proof}
The system \eqref{eq:moment_system} represents optimality conditions to the minimization problem
\begin{equation}
    \label{eq:min_problem}
    \text{minimize}(\vc\lambda):\quad F(\vl) = \int_{\Omega} \rho_{\vl} - \vl\cdot\vmu.
\end{equation}
The functional is strictly convex due to the strict convexity of $e^t$ (note also that $\tn H$ is SPD Hessian of $F$). 
Therefore the problem \eqref{eq:min_problem} possesses a unique global minimum $\vl(\vmu)$ which is also solution to the system \eqref{eq:moment_system}.


\subsection{Projection to exponential family}

Following \cite{Barron1991}, we can define set of densities with given moments $\vmu$:
\[
    C_{\vmu} = \{ \rho : \E_\rho[\vphi] = \vmu \}
\]
as well as the exponential family of densities:
\[
    \mathcal F_{p} = \{ \rho_\vl = 
    p e^{\vl\cdot\vphi} :
    \vl \in \Lambda\},\quad \Lambda = 
    \{\vl\in\R^M,\ \int_\Omega p e^{\vl\cdot\vphi} = 1\}.
\]
Then we can follow \cite[Lemma 2]{Barron1991} to get decomposition:
\begin{lemma}
\label{thm:decomposition}
For given $\vmu \in \mathcal M$ denote $\vl =\vl(\vmu)$ the unique solution to the system \eqref{eq:moment_system}. Then for any $\rho \in C_{\vmu}$ and $\estrho=\rho_\estvl \in\mathcal F_p$ the decomposition holds:
\begin{equation}
    \label{eq:decomposition}
    D(\rho\Vert\estrho) = D(\rho\Vert\rho_{\vl}) + D(\rho_{\vl}\Vert
    \estrho).
\end{equation}
In particular $\rho_{\vl}$ is minimizer of $D(\rho\Vert\rho_{\vl})$ over $\mathcal F_p$.
\end{lemma}
\begin{proof}
Due to \eqref{eq:moment_system} it holds $\E_{\rho}[\vphi] = \E_{\rho_{\vl}}[\vphi] = \vmu$. Then straightforward application of this identity yields:
\[
 D(\rho\Vert\estrho) = D(\rho\Vert\rho_{\vl}) 
 + \int_\Omega \rho (\vl - \estvl)\cdot \vc \phi = D(\rho\Vert\rho_{\vl}) + D(\rho_{\vl}\Vert\estrho).
\]
\end{proof}
The first term on the right side of \eqref{eq:decomposition} corresponds to the approximation error of the family $\mathcal F_p$ while the second term with $\estvl = \estvl(\estvmu)$ represents the error due to the moment estimation. 

\subsection{Approximation error}
Following estimates are based on \cite[Lemma 2]{Barron1991} 
\begin{lemma} 
  \begin{equation}
      \label{eq:l2_estimate}
      D(p\Vert q) \le \frac{1}{2}e^{\norm{\log p/q - c}_\infty} \norm{\log p/ q - c}_{L^2_p}^2
  \end{equation}

  
  \begin{equation}
      \label{eq:l1_estimate}
      D(p\Vert q) \le \frac{1}{2}\norm{\log p/ q}^2_{L^2_p} + \norm{\log p/ q}_{L^1_p}
  \end{equation}
 for any constant $c$.
\end{lemma}

Then for the approximation error, we have:
\begin{lemma}
\label{thm:approx_error}
For given approximation space $\mathcal V_M$ and density $\rho\in C_{\vmu}$ let us denote
\[
  \epsilon = \epsilon_{M,\infty}(\rho) = \inf_{\vphi \in \mathcal V_M} \norm{\log \rho - \phi}_\infty.
\]
Then for the approximation error, we have estimate:
\[
D(\rho\Vert\rho_{\vl})  \le \frac{\abs{\Omega}}{2} e^{\epsilon} \epsilon^2.
\]
\end{lemma}
\todo{Jak je to s volbou $M$, z tohoto lematu by clovek rekl pokud zvolim dost vysoke $M$ tak mohu aproximovat libovolne presne a kovergence je rychla. Na druhou z numerickych testu to vypada, ze pro vetsi $M$ dostavame velke oscilace v hustote. Jedno vysvetleni je, ze KL divergence muze byt mal i pro oscilujici aproximaci hustoty. Potrebujeme numericke testy pro presne hustoty.} 
\begin{proof}
Lemma \ref{thm:decomposition} states:
\[
D(\rho\Vert\rho_{\vl}) \le \inf_{\estvl \in \Lambda} D(\rho\Vert\rho_{\estvl})
\]
Now for any $\vc{\tilde\lambda} \in \R^M$, we can modify the first component to find $\estvl \in \Lambda$ satisfying
\[
  \estvl \cdot \vphi = \vc{\tilde\lambda} \cdot \vphi - \psi, \quad \psi 
                             = \log \int_\Omega p e^{\widetilde{\vc\lambda}\cdot \vc \phi}
\]
With such choice we have:
\[
D(\rho\Vert\rho_{\vl}) \le \inf_{\estvl \in \Lambda} D(\rho\Vert\rho_{\estvl})
=\inf_{\vc{\tilde\lambda} \in \R^M} D(\rho\Vert q_{\vc{\tilde\lambda}})
\]
Applying \eqref{eq:l2_estimate} with $c = \log p - \psi$, we obtain:
\[
  D(\rho\Vert q_{\vc{\tilde\lambda}}) \le 
  \frac{1}{2}
  e^{\norm{\log \rho - \vc{\tilde\lambda}\cdot \vc \phi}_\infty} 
    \norm{\log \rho - \vc{\tilde\lambda}\cdot \vc \phi}_{L^2_p}^2
  \le \frac{\abs{\Omega}}{2} e^{\epsilon} \epsilon^2.
\]
\end{proof}
For the case of polynomial approximation $\mathcal V_M = P^M(\Omega)$ we can directly apply results form \cite[Section 7]{Barron1991}. For a $\rho \in W^r_2(\Omega)$:
\[
    \epsilon_{m,\infty}(\rho) \le C \left(\frac{1}{m}\right)^{r-1} \norm{\rho}_{W^r_2},
\]
\[
    \epsilon_{m,2}(\rho) \le C \left(\frac{1}{m}\right)^{r}\norm{\rho}_{W^r_2}.
\]


\begin{figure}[!htb]
\begin{center} 
\includegraphics[width=0.75\textwidth]{mu_err_iterations.pdf}
\end{center} 
\caption{Convergence of the minimization method with preconditioning, M = $85$.}
\label{fig:convergence}
\end{figure} 
\FloatBarrier
The figure \ref{fig:convergence} describes the convergence of numerical minimization. Distributions are configured as described in section \ref{benchmarks_distr}. The number of 85 exact moments is utilized. Given our preconditioning (cutting eigenvalues below or equal to zero), the truncation of moments occur in cases of five-fingers ($M=81$) and zero-value ($M=57$) distributions.

\begin{figure}[!htb]
\begin{center} 
\includegraphics[width=0.75\textwidth]{mu_err_iterations_no_precond.pdf}
\end{center} 
\caption{Convergence of the minimization method without preconditioning, $M = 85$.}
\label{fig:convergence}
\end{figure} 
\FloatBarrier
\todo{Preconditioning seems to even get worse convergence!?}

\subsection{Benchmark distributions}\label{benchmark_distr}

Following representative distributions are chosen to verify our assumptions. The majority of them is selected based on the article by J. Farmer and D. Jacobs \cite{Farmer2018511}.

% Normal distribution $f(x) = \mathcal{N}(x|\mu=0, \sigma=10) \text{, } x \in (-\infty, \infty)$ is the basic example and has propitious features. In terms of MEM, it is possible to approximate its shape \ref{fig:normal} with one exponential. 

Two-Gaussians \ref{fig:two-gaussians} is composition of two normal distributions \newline ${\rho(x) = \frac{7}{10} \mathcal{N}(x|\mu_{1}=5, \sigma_{1}=3) + \frac{3}{10}\mathcal{N}(x|\mu_{2}=0, \sigma_{2}=0.5)}$, $x \in (-\infty, \infty)$. In this case it is unable to employ just one exponential to capture exact shape of the small peak on the side of the higher one.
To further challenge our approach we implement a composition of very steep and close normal distributions called five-fingers distribution \ref{fig:five-fingers}
$\rho(x) = \sum_{k=1}^{5}{\mathcal{N}\bigg(x\bigg|\mu_{k}=\frac{2k-1}{10}, \sigma_{k} = \frac{1}{100}\bigg)}$, $x \in [0, 1]$. 
The Cauchy distribution $\rho(x) = \frac{1}{\pi(x^2 + 1)} \text{, } x \in [-20, 20]$ which has a heavy tail \ref{fig:cauchy} and a non existing standard deviation is truncated to ensure stability of MEM and existence of solution.
Due to the further application we also investigate MEM ability to capture lognormal distribution with PDF $\rho(x) = \mathcal{LN}(x|\mu=4.5, \sigma=5.9) \text{, } x \in (0, \infty)$.

The Abyss distribution \ref{fig:abyss} incorporate points of discontinuity. It is not possible to fully capture this distribution by polynomials that have wide support. 
\begin{equation*}
    \rho(x)=
    \begin{cases}
      0.1, & -0.1 \geq x \leq 0.1 \\   
      \mathcal{N}(x|\mu=0, \sigma=1), & \text{elsewhere.}
    \end{cases}
\end{equation*}
The last distribution that is called zero-value is considered on the interval $x \in [-1, 1]$ with the following PDF
\begin{equation*}
    \rho(x)=
    \begin{cases}
      0, & x \leq 0.8 \\   
      50x-40, & \text{elsewhere.}
    \end{cases}
\end{equation*}
Distributions that do not have an explicitly defined interval on which they are considered are implicitly limited to the interval between quantiles $0.001$ and $0.999$.

% \begin{figure}[htb]
%     \centering % <-- added
% \begin{subfigure}{0.5\textwidth}
%   \includegraphics[width=\linewidth]{benchmark_distributions/approx_error/two-gaussians_exact.pdf}
%   \caption{two-gaussians}
%   \label{fig:two-gaussians}
% \end{subfigure}\hfil % <-- added
% \begin{subfigure}{0.5\textwidth}
%   \includegraphics[width=\linewidth]{benchmark_distributions/approx_error/five-fingers_exact.pdf}
%   \caption{five-fingers}
%   \label{fig:five-fingers}
% \end{subfigure}\hfil
% \centering
% \begin{subfigure}{0.5\textwidth}
%   \includegraphics[width=\linewidth]{benchmark_distributions/approx_error/cauchy_exact.pdf}
%   \caption{Cauchy}
%   \label{fig:cauchy}
% \end{subfigure} \hfil% <-- added
% \centering
% \begin{subfigure}{0.5\textwidth}
%   \includegraphics[width=\linewidth]{benchmark_distributions/approx_error/lognorm_exact.pdf}
%   \caption{log-normal}
%   \label{fig:discontinuous}
% \end{subfigure}\hfil % <-- added

% \begin{subfigure}{0.5\textwidth}
%   \includegraphics[width=\linewidth]{benchmark_distributions/approx_error/abyss_exact.pdf}
%   \caption{abyss}
%   \label{fig:abyss}
% \end{subfigure}\hfil
% \begin{subfigure}{0.5\textwidth}
%   \includegraphics[width=\linewidth]{benchmark_distributions/rho4.pdf}
%   \caption{$\rho^4$}
%   \label{fig:rho4}
% \end{subfigure}\hfil % <-- added
% \caption{Approximation error}
% \label{fig:approx_error}
% \end{figure}
% \FloatBarrier



\begin{figure}[htb]
    \centering % <-- added
\begin{subfigure}{0.333\textwidth}
  \includegraphics[width=\linewidth]{benchmark_distributions/two_gaussians.pdf}
  \caption{two-gaussians}
  \label{fig:two-gaussians}
\end{subfigure}\hfil % <-- added
\begin{subfigure}{0.333\textwidth}
  \includegraphics[width=\linewidth]{benchmark_distributions/five_fingers.pdf}
  \caption{five-fingers}
  \label{fig:five-fingers}
\end{subfigure}\hfil
\begin{subfigure}{0.333\textwidth}
  \includegraphics[width=\linewidth]{benchmark_distributions/cauchy.pdf}
  \caption{Cauchy}
  \label{fig:cauchy}
\end{subfigure} % <-- added
\medskip
\begin{subfigure}{0.333\textwidth}
  \includegraphics[width=\linewidth]{benchmark_distributions/lognorm.pdf}
  \caption{lognormal}
  \label{fig:lognormal}
\end{subfigure}\hfil % <-- added
\begin{subfigure}{0.333\textwidth}
  \includegraphics[width=\linewidth]{benchmark_distributions/abyss.pdf}
  \caption{abyss}
  \label{fig:abyss}
\end{subfigure}\hfil
\begin{subfigure}{0.333\textwidth}
  \includegraphics[width=\linewidth]{benchmark_distributions/zero_value.pdf}
  \caption{zero-value}
  \label{fig:rho4}
\end{subfigure}\hfil % <-- added
\caption{Benchmark distributions}
\label{fig:images}
\end{figure}
\FloatBarrier


\subsection{Estimation error}
We start with a sensitivity estimate for the nonlinear system \eqref{eq:moment_system}.

\begin{lemma}
\label{thm:lambda_est}
Let $\estvl(\estvmu)$ be the solution of the system \eqref{eq:moment_system} for an approximation 
$\estvmu \in \mathcal M$ of the moments \eqref{eq:gen_moments} and let $\vl(\vmu)$ be the solution 
for exact moments $\vmu$. Then
\begin{equation}
\label{eq:lambda_mu}
   \abs{\vl - \estvl} \le \frac{1}{\alpha_0} \abs{\vmu - \estvmu} 
\end{equation}
where $\alpha_0$ is the smallest eigenvalue of the Hessian matrix $\tn H(\estvl)$
\end{lemma}
\begin{proof}
Unfortunately we are yet unable to proof this result. So we try some numerical tests.

Based on numerical experiments, we try to show that the relation \ref{eq:lambda_mu} holds true for all our benchmark distributions. All distributions are restricted on the interval $[Q_{0.01}, Q_{0.99}] $, $M=25$ is utilized.
 The results are depicted in the picture \ref{fig:lambda_kl_tv}. In graphic terms, the formula \ref{eq:lambda_mu} is valid if all red dots with coordinates ($|\vmu - \estvmu|, \alpha_{0}|\vl - \estvl|$) are on or below the black line. The line represents the axis of the first quadrant. 


The data for the six benchmark distributions suggest the validity of lemma \ref{thm:lambda_est} at least when the moment error $|\vmu - \estvmu|$ is not too large.
\begin{figure}[!htb]
\begin{center} 
\includegraphics[width=\textwidth]{lambda_kl_tv.pdf}
\end{center} 
\caption{Estimation error}
\label{fig:lambda_kl_tv}
\end{figure} 

Further numerical tests confirm that the eigenvalues of the Hessian $\tn H(\estvl)$ are usually close to one. And smallest eigenvalue seems to increase as $\estvl$ tends to $\vl$. 
$\tn H(\estvl)$ should be very close to one, but it is not the case for some test distributions, but the limit value is somewhat smaller. Possibly that could be caused by insufficient normalization. 
However it seems that the lambda bound holds even without renormalization.



In addition to verifying the validity of the formula \ref{eq:lambda_mu}, we also tried to show the KL divergence and its relation to the total variation \note{It is total variation of two  distributions, it is not total variation of single distribution - roughness of distribution} and has the following general form:
$$
TV(p\Vert q) = \frac{1}{2}\Vert p(x) - q(x)\Vert_1.
$$ 

Pinker's inequality bounds total variation from above in terms of the KL divergence: % (see http://people.lids.mit.edu/yp/homepage/data/LN_fdiv.pdf, https://arxiv.org/pdf/1806.11311.pdf)
\begin{equation}
D(\rho_{\vl}\Vert\estrho) \geq (2\log e) {TV}^2(\rho_{\vl}\Vert\estrho).
\end{equation}

\note{KL divergence $D(\rho_{\vl}\Vert\estrho)$ was calculated as $D(\rho_{\vl}\Vert\estrho) = (\vl - \hat{\vl})\vmu$}

To depict this relation we added
the blue dots with coordinates: $(|\vmu - \estvmu|, \sqrt{\frac{D(\rho_{\vl}\Vert\estrho)}{C_M}})$
and the green dots $(|\vmu - \estvmu|, \sqrt{\frac{(2\log e) {TV}^2(\rho_{\vl}\Vert\estrho)}{C_M}})$.


Where the blue dots y-axis coordinates are based on KL divergence estimation error upper bound $$D(\rho_{\lambda} \Vert \hat{\rho}) \leq C_M \Vert \vmu - \hat{\vmu}\Vert^2,$$ where $C_M = 2\exp\{\Vert log(\rho / \rho_{\vl}) \Vert_{L^{\infty}} + 1\}$, for details see \cite[p. 1358]{Barron1991}.

Given the KL divergence total variation comparison, we can assume that with a decreasing $D(\rho_{\lambda} \Vert \hat{\rho})$, the ${TV}(\rho_{\vl}\Vert\estrho)$ will also decrease and it should help reduce the roughness of $\hat{\rho}$.

% \includegraphics[width=\textwidth]{mu_to_lambda.pdf}

% The blue dots have coordinates: $(|\vmu - \estvmu|, |\vl - \estvl|)$, blue lines are estimates for bounds from \cite{Barron1991}. The red dots are $(|\vmu - \estvmu|, \alpha_0|\vl - \estvl|)$,
% red line have slope $\alpha_0$. The results for the six test distributions suggest validity of our estimate at least when the moment error $|\vmu - \estvmu|$ is not to large.

% Further numerical tests confirms that the eigenvalues of the Hessian $\tn H(\estvl)$ are usually close to one. And smallest eigenvalue seems to increase as $\estvl$ tends to $\vl$. 
% $\tn H(\estvl)$ should be very close to one, but it is not the case for some test distributions, but the limit value is somewhat smaller. Possibly that could be caused by insuficient normalization. 
% However it seems that the lambda bound holds even without renormalization.

Here, we present some partial results and observations.
Let us denote $a=\vl\cdot\vphi$ and $b = \estvl\cdot\vphi$. We start with:
\[
  (\vl - \estvl)\cdot(\vmu - \estvmu)
      = \int_{\Omega} p \big[(\vl - \estvl)\cdot\vphi\big] (e^{(\vl\cdot\vphi} - e^{\estvl\cdot\vphi})
      =\int_{\Omega} p (a - b)(e^a - e^b)
\]

The next step is usage of $sinh$ expansion:
\[
 e^a - e^b = 2 e^{\frac{a+b}{2}}\frac{1}{2}\Big[e^{\frac{a-b}{2}} - e^{\frac{b-a}{2}}\Big] = 
 2 e^{\frac{a+b}{2}}\sinh(\frac{a-b}{2}),
\]
considering the expansion:
\[
    \sinh(x) = x + \frac{x^3}{6} + \frac{x^5}{120} + \dots
\]
we get an estimate
 \[
  (a-b)(e^a - e^b) \ge e^{\frac{a+b}{2}}\Big[|a-b|^2 + \frac{1}{24}|a-b|^4 + \dots\Big].
 \]

 All terms on the right have sign. Using just the first term, get a result:
\[
  (\vl - \estvl)\cdot(\vmu - \estvmu) \ge \int_{\Omega} p (a - b)^2(e^{a+b/2})
   \ge \frac{|\vl - \estvl|^2}{\alpha_0((\vl + \estvl)/2)}
\]
where $\alpha_0((\vl + \estvl)/2)$ is the smallest eigen value of the Hass matrix
$\tn H((\vl + \estvl)/2)$. This is not so good as we only know $\estvl$. 

 
 
 


% 
% The estimate starts with
% 
% Due to convexity of $f(t) = e^t$, we have
% \[
%     \frac{e^b - e^a}{b-a} \ge f'(\min(a,b))  = e^{\min(a,b)}
% \]
% for any $a, b\in \R$. Multiplying by $(b-a)^2\ge 0$ and using $\min(a,b) \ge a - |a-b|$, we get
% \begin{equation}
%   \label{eq:exp_ineq}
%   (b-a)(e^b - e^a) \ge e^a e^{-|a-b|}(b-a)^2 
% \end{equation}
%  
% 
% 
% Now we apply \eqref{eq:exp_ineq} with $a = \estvl\cdot\vphi$ and $b=\vl\cdot\vphi$ to estimate:
% \[
%   (\vl - \estvl)\cdot(\vmu - \estvmu)
%       = \int_{\Omega} p \big[(\vl - \estvl)\cdot\vphi\big] (e^{\vl\cdot\vphi} - e^{\estvl\cdot\vphi})
%       \ge \int_\Omega \big[(\vl - \estvl)\cdot\vphi\big]^2 pe^{\min(\estvl\cdot\vphi, \vl\cdot\vphi)}
% \]
% Further we can use:
% \[
%  \min(a, b) = \frac{a + b}{2} - \frac{|a - b|}{2}
% \]
% or 
% \[
%  \min(a, b) \ge a  - |a - b|.
% \]
% 
% Using $\sinh$:
% \[
%  e^a - e^b = 2 e^{\frac{a+b}{2}}\frac{1}{2}\Big[e^{\frac{a-b}{2}} - e^{\frac{b-a}{2}}\Big] = 
%  2 e^{\frac{a+b}{2}}\sinh(\frac{a-b}{2}),
% \]
% considering the expansion:
% \[
%     \sinh(x) = x + \frac{x^3}{6} + \frac{x^5}{120} + \dots
% \]
% 
%  
% Estimate according to Barron, Sheu \cite{Barron1991}. Lemma 5. 
% \[
%     |\vl_0 - \vl_r|^2 \le 2e^M e^\tau |\vl_0 - \vl_r| |\mu_0 - \mu_r|; \quad M = |\log  q/ \rho_0 |    
% \]
% 
%  
 
% We start our estimate with realization:
% \[
%     F_{\estvmu}(\vl_0) - F_{\estvmu}(\vl_r)  = -\mu_r(\vl_0 - \vl_r) \ge 0
% \]
% 
% \[
%     F_{\vmu}(\vl_r) - F_{\vmu}(\vl_0)  = -\mu_0(\vl_r - \vl_0) \ge 0
% \]

% \begin{align*}
%   (\mu_r - \mu_0) \cdot (\vl_r - \vl_0) &- \frac12 e^{-\tau} (\vl_0 - \vl_r) \tn H(\vl_r) (\vl_0 - \vl_r) \\
%   &=
%   \int_\Omega \Big[ e^{\vl_r\cdot\vphi} - e^{\vl_0\cdot\vphi} \Big] (\vl_r - \vl_0) \cdot \vphi 
%   -\int_\Omega  \frac12\big[ (\vl_0 - \vl_r) \cdot \vphi \big]^2 e^{\vl_r\cdot\vphi -\tau}\\
%   &\ge \frac12\int_\Omega  \big[ (\vl_0 - \vl_r) \cdot \vphi \big]^2 \big[e^\frac{(\vl_r + \vl_0) \cdot \vphi}{2} - e^{\vl_r\cdot\vphi-\tau}\big]
% \end{align*}

Applying this we get:
\[
  (\mu_r - \mu_0) \cdot (\vl_r - \vl_0) =
  \int_\Omega \Big[ e^{\vl_r\cdot\vphi} - e^{\vl_0\cdot\vphi} \Big] (\vl_r - \vl_0) \cdot \vphi 
  \ge \int_\Omega  \big[ (\vl_0 - \vl_r) \cdot \vphi \big]^2 e^\frac{(\vl_r + \vl_0) \cdot \vphi}{2}
\]
or more precisely:
\[
  (\mu_r - \mu_0) \cdot (\vl_r - \vl_0) =
  \int_\Omega  \big[ (\vl_0 - \vl_r) \cdot \vphi \big]^2 e^{\vl_r\cdot\vphi} e^\frac{(\vl_0 - \vl_r) \cdot \vphi}{2} S(x)
\]
where
\[
  S(x) = \sum_{n=0}^\infty \frac{|(\vl_0 - \vl_r) \cdot \vphi|^{2n}}{(2n+1)!2^{2n}}.
\]





Further we can estimate:
\[
    e^\frac{a + b}{2} \ge e^{a} - \abs{e^\frac{a + b}{2} - e^{a}}
\]
and then:
\[
 \abs{e^\frac{a + b}{2} - e^{a}} = 2e^\frac{3a+b}{4} \sinh\Big(\frac{|b-a|}{4}\Big)
\]

For any $0 \le x \le \tau$ we have $\sinh(x) \le \sinh(\tau) x$, thus
\[
 \abs{e^\frac{a + b}{2} - e^{a}} \le 2e^\frac{3a+b}{4} \sinh(\tau) \frac{|b-a|}{4}
 =  \sqrt{2\epsilon} e^{a/2}|a-b| \frac{\sinh(\tau)}{2\sqrt{2\epsilon}} e^{\frac{a+b}{4}}  \le 
    \epsilon e^{a}|a-b|^2 +  C e^{\frac{a+b}{2}}
\]
with $C = \frac{\sinh^2(\tau)}{16\epsilon}$.
combining together:
\begin{align*}
 (\mu_r - \mu_0) \cdot (\vl_r - \vl_0) 
 \ge (1-\epsilon)\int_\Omega  \big[ (\vl_0 - \vl_r) \cdot \vphi \big]^2 e^{\vl_r \cdot \vphi}
 -C \int_\Omega e^\frac{(\vl_r + \vl_0) \cdot \vphi}{2}
\end{align*}
However, the last term is 











\[
 e^\frac{(\vl_r + \vl_0) \cdot \vphi}{2} - e^{\vl_r\cdot\vphi-\tau} \ge 
\]





\begin{align*}
  \alpha_0 e^\abs{\vl - \estvl}^2 &\le  (\vl-\estvl)^T\tn H(\estvl)(\vl -\estvl)\\
  &= \int_\Omega (\vl\cdot\vphi  - \estvl\cdot\vphi)^2 pe^{\estvl\cdot\vphi}
  \le \int_{\Omega} (\vl\cdot\vphi - \estvl\cdot\vphi) 
  p(e^{\vl\cdot\vphi} - e^{\estvl\cdot\vphi})\\
  &=  (\vl - \estvl)\cdot(\vmu - \estvmu) \le \abs{\vl - \estvl}\abs{\vmu - \estvmu}
\end{align*}
\end{proof}




Then for the estimation error we have following result.
\begin{lemma}
  \label{thm:est_error}
  Let $\rho_\lambda \in \mathcal F_p \cap C_{\vmu}$ for a $\vmu \in \mathcal M$, 
  and $\estrho = \rho_{\estvl}\in \mathcal F_p$ 
  where $\estvl$ is solution of \eqref{eq:moment_system} for $\estvmu \in \mathcal M$. Then 
  \[
    D(\rho_{\vl}\Vert \estrho) \le \frac{1}{\alpha_0} \abs{\vmu - \estvmu}^2
  \]
\end{lemma}

\begin{proof}
For any $\tvl$, we denote (c.f. \eqref{eq:min_problem}):
\[
F_{\estvmu}(\tvl) = \int_{\Omega} \rho_{\tvl} - \tvl\cdot\estvmu.
\]
In particular for $\vl$ that is solution of \eqref{eq:moment_system} with $\mu_1 = 1$, we have:
$F_{\estvmu}(\vl) = 1 - \vl\cdot\estvmu$. Similarly for $\estvl$ holds
$F_{\estvmu}(\estvl) = 1 - \estvl\cdot\estvmu$. Moreover, since $\estvl$ is minimizer 
of $F_{\estvmu}$, we conclude:
\[
 0\le F_{\estvmu}(\vl) - F_{\estvmu}(\estvl) = - \estvmu\cdot(\vl - \estvl).
\]
Now we can rewrite the KL-divergence as:
\[
  D(\rho_{\vl}\Vert \estrho) = (\vl - \estvl)\cdot \vmu \le
  (\vl - \estvl)\cdot (\vmu - \estvmu)
\]
Finally applying Lemma \ref{thm:lambda_est}
\[
D(\rho_{\vl}\Vert \estrho) \le \frac{1}{\alpha_0} \abs{\mu - \mu^*}^2
\]
 
\end{proof}
Similarly as in \cite[Theorem 3]{Barron1991} we  
\begin{theorem}
  \label{eq:estimate_err_var}
  Let $\rho_{\vl} \in \mathcal F_p \cap C_{\vmu}$ be the projection of the exact density $\rho$ to the exponential family based on exact moments  $\vmu = \E_\rho \vphi$. The MLMC estimator \eqref{eq:mlmc_est_moments} provides estimated moments $\estvmu$, we denote $\estrho = \rho_{\estvl}\in \mathcal F_p$ corresponding density with parameters $\estvl$ given by \eqref{eq:moment_system}.
  Assuming knowledge of the error $V = \E_{\rho} |\vmu - \estvmu|^2$, then for given (small) probability $\pi$
  we have
  \[
      P_{\rho}\Big( D(\rho_{\lambda}\Vert \estrho) \le \eta\Big) \ge 1 - \pi,\quad 
      \text{with } \eta = \frac{V}{\alpha_0 \pi},
  \]
  where $\alpha_0$ is the smallest eigenvalue of the Hessian matrix $\tn H(\estvl)$.
  
\end{theorem}
\begin{proof}
  Using Lemma \ref{thm:est_error} and applying Markov's inequality unbiased estimate $\estvmu$ we have:
  \[
   P\Big( D(\rho_{\lambda}\Vert \estrho) \ge \eta\Big) 
   \le P\Big( \abs{\vmu - \estvmu}^2 \ge \alpha_0  \eta\Big)
   \le \frac{\E_{\rho} |\vmu - \estvmu|^2}{\alpha_0 \eta} = \pi
  \]
\end{proof}


\todo{TODO:}

Cite estimates for $D(\rho\Vert\rho_{\lambda})$.

Modify estimates for $D(\rho\Vert\rho_{\lambda})$ using $q=p$ and
approximation of $Cov_p \vc phi$ for Legendere polynoms $\vc\phi$.

Probabilistic result $D(\rho_{\lambda*} \Vert \rho_{\lambda}) < V/p$
with probability at least $1-p$, where 
\[
    V = \sum_r \sum_l \frac{1}{N_l} \Var \delta^l\phi_r \approx \sum_r \sum_l \frac{1}{N_l} \hat{\Var} \delta^l\phi_r 
\]

\todo{Plot KL divergence between approximations for different $M$. We should have a drop in error up to level where the Estimation error dominates.}
\todo{Choice of basis leading to $\alpha_0 \approx 1$.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Nonlinear solver}\label{nonlinear_solver}

In this section we shall discus numerical solution of the minimization problem \eqref{eq:min_problem}. We prefer this setting over the direct solution of the nonlinear system \eqref{eq:moment_system} as it allows us to prescribe suitable regularization and penalization terms to improve the stability.
The problem is poorly conditioned unless we choose a basis in which $\tn H(\estvl)$ is close to identity matrix. Crucial observation is that $\tn H(\estvl)$ should be close to the non-centered 
covariance matrix of  $\vphi(X)$. In particular, if $\hat X$ is a random variable with PDF $\rho_{\estvl}$, then $\tn M \tn H(\estvl) \tn M^T = \Cov \vphi(\hat X)$, where 
\[
  \tn M = 
  \begin{pmatrix}
    -1    & 0 &\dots &0\\
    -\hat\mu_2    & 1 &\dots &0\\
    \vdots& 0 &\ddots      &\vdots\\
    -\hat\mu_n    & 0 &\dots &1
  \end{pmatrix}.
\]
Centering matrix $\tn M$ is so called \emph{involutory matrix}, i.e. $\tn M^{-1} = \tn M$.


Since $\rho_{\estvl}$ is approximation of $\rho$, the covariance of $\vphi(\hat X)$ should be close to the covariance of $\vphi(X)$ which can be estimated be the MLMC estimator \eqref{eq:mlmc_est_cov}.

In order to exploit these observations, we first choose an initial set of linearly independent 
moment functions $\phi_m$, $m=1,\dots,M$ with $\phi_1=1$. These form a base of the space $\mathcal V_M$.
Next, we compute the estimate $\hat{\tn C}$ of the covariance matrix using the estimator \eqref{eq:mlmc_est_cov}. The estimated matrix is still symmetric but need not be positive definite
due to errors on individual levels of the MLMC estimator. 


Estimator gives non-central covariance estimate:
\[
 \avg{H}. 
\]
Then we compute the covariance approximation as: $\hat{\tn C} = \tn M \avg{H} \tn M^T$.
Then we compute eigenvalue decomposition $\hat{\tn C}= P^T \Lambda P$. 
Then we drop negative eigenvalues and in the remaining sequence
of decreasing values $\log \lambda_i$ we detect a construct a linear model for a floating window of eigenvalues and detect a point of rapid decrease of the values (Figure). Let us denote $N$ the number of remaining eigenvalues and eigenvectors. Having eigenvalues $\lambda_1 \dots \lambda_N$ and $M\times N$ matrix $Q$ of remaining eigen vectors in columns. It still holds $Q^T Q = I$ with $[N\times N]$. 
We have:
\[
   \tn H \approx M^T Q\Lambda' Q^TM
\]

We prescribe reduced set of moment functions as:
\[
   \tilde\vphi =  \tn L \vphi, \quad \tn L =  \tn R \Lambda^{-\frac12} Q^T \tn M^T
\]
for any orthonormal matrix $\tn R$. 
Now we have for $\hat{\tn H}$ ($N\times N$):
\begin{align*}
  \hat{\tn H} &= \E_{\estrho}\big[ \tilde\vphi \otimes \tilde\vphi\big] 
              = \tn L \E_{\estrho} \big[\vphi \otimes \vphi \big] \tn L^T 
              \approx \tn L \avg{\tn H} \tn L^T = \tn L \tn M^T \tn P \Lambda \tn P^T \tn M \tn L^T \\
              &\approx \tn L \tn M^T \tn Q \Lambda^{\frac12} \Lambda^{\frac12} \tn Q^T \tn M \tn L^T  = I 
\end{align*}


\todo{Ucesat}
\todo{Graf s poklesem vlastnich cisel}
\todo{Graf $\vphi$ for vlastni vektory.}

\begin{figure}[!htp]
\centering
\includegraphics[width=0.5\linewidth]{eigenvalues.pdf}
\caption{Eigenvalues of $\hat{\tn C}$. Those that are below the threshold (vertical line) are removed. $\sigma^2$ represents moments variance which is also the threshold value.}
\label{fig:eigenvalues}
\centering
\end{figure}
\FloatBarrier


\section{Choice of moments}
\subsection{Monomials}
The first choice for moment functions are monomials:
\[
    \phi_r(x) = x^r
\]
corresponding to classical moments.
In order to improve conditioning of the problem we should use centralized moments:
\begin{align}
    \phi_0(x) &= 1\\
    \phi_1(x) &= x - \mu_1\\
    \phi_r(x) &= \phi_r(x - \mu_1), \text{ for }r=1\dots R
\end{align}
with $\mu_0 = 1$.

However as the entropy function $e_n(x) = -\ln(\rho_n)$ is a linear combination of the generalized moment functions, it may expect that using monomials leads to badly conditioned non-linear problem. In order to 
get better conditioning needs to choose moment functions to be nearly orthogonal and possibly with small support.

\subsection{Fourier moment functions}


\subsection{Spline approximation}
In order to keep Jacobi matrix well conditioned can keep supports of $\phi_r$ to be mostly disjoint since if $supp \phi_r$ is disjoint with $supp \phi_s$ we have $H_{rs} = 0$.
In order to keep basic smoothness, we may use spline base functions, in particular cubic splines.



\section{Synthetic experiments}
\section{Correlated random fields}
We what to generate realizations of a random field: $Z(\vc x, \omega)$, for $\vc x$ in domain $D \subset R^d$. We consider {\it second order field}, i.e. with finite variance for every $\vc x\in D$. For such fields we can define mean:
\[
    \mu_Z(\vc x) = \E\big[ Z(\vc x) \big]
\]
and covariance:
\[
    C_Z(\vc x, \vc y) = \E\big[ (Z(\vc x) - \mu(\vc x))(Z(\vc y) - \mu(\vc y)) \big]
\]

We restrict ourselves to the case of stationary fields, where $C$ depends only on $(\vc x - \vc y)$. Our aim is to compute lot of realizations of the field fro a fixed set of points. Further on, we focus on the zero-mean fields:
\[
    \tilde Z(\vc x) = Z(\vc x) - \mu(\vc x), \E(\tilde Z) = 0.
\]

TODO:
\begin{itemize}
\item Approximation of quantiles or distribution function via. density is not optimal, since the error is accumulated through the integration of the density. Consider approximation of distribution via. approximation of Havyside function (see Giles), propose some approximation for quantiles.

\item Derive error estimates for density, quantiles, distribution  for various set of moments. How to approximate tails? Is the maximum entropy the best choice?

\item How to use MLMC for generated random fractures, in particular how to make two generated fracture sets that are "correlated".
\end{itemize}


\section{A sampling of Random Fields}
This section is devoted to the problem of generating cross-correlated random fields.

\subsection{Random Field}
Any scalar random field $u(\vc x)$ with mean field $\mu(\vc x)$ and standard error
field $\sigma(\vc x)$ can be normalized as:
\[
    u(\vc x) = \mu(\vc x) + \sigma(\vc x) \tilde{u}(\vc x)
\]
where $\tilde{u}$ is a \emph{normalized} random field with zero mean and variance equal to $1$. Therefore we shall discuss only generation of normalized random fields further on. The normalized random field is given by its correlation function:
\[
    {\rm Cov}(u(\vc x), u(\vc y) ) = c(\vc x, \vc y)  
    = \E\big[ (u(\vc x) - \E u(\vc x))(u(\vc y) - \E u(\vc y)) \big]
\]
The field is called \emph{stationary} if $c(\vc x, \vc y) = c(\vc x - \vc y)$, i.e. 
correlation function depends on direction vector but is invariant to changes in position. The field is called \emph{isotropic} if $c(\vc x, \vc y) = c(|\vc x - \vc y|)$, i.e. it is also invariant with respect to rotations. 

The Wiener-Kintchine theorem states that
\cite{}

\subsection{Variogram function}
Instead of correlation function we can use the \emph{variogram function} $\gamma$:
\[
    \gamma(u(\vc x), u(\vc y)) = \frac12 \E\big[u(\vc x) - u(\vc y)\big].
\]
General relation ship between $c$ and $\gamma$ is quite complicated, but if we assume $\E u(\vc x) = 0$, then
we have:
\[
    \gamma(\vc x, \vc y) = \frac12 \big[ c(\vc x, \vc x) + c(\vc y, \vc y)\big] - c(\vc x, \vc y)
\]
and in the case of normalized random field:
\[
    \gamma(\vc x, \vc y) = 1 - c(\vc x, \vc y)
\]


There are two kinds of methods: decomposition and spectral methods. The decomposition methods use singular value decomposition of the correlation matrix for a fixed set of evaluation points. And use incomplete factorization of the Karhunen-Lo\`eve expansion:
\[
    Z(\vc x, \omega) = \mu(\vc x) +  \sum_{i = 1}^\infty \sqrt{\lambda_i} \xi_i(\omega) \phi_i(\vc x)
\]
where  $\xi_i$ are uncorrelated random variables, $\lambda_i$ are the eigenvalues and  $\phi_i$ are 
the eigenfunctions of the correlation operator. In the case of fixed evaluation points, we have just a finite
dimension, the correlation operator is a correlation matrix and sum is finite. As the eigenvalues usually decrease
rapidly we can use truncated sum, i.e. truncated KL expansion.

The spectral methods sample the field by its Fourier Second kind of methods use sp

\section{Numerical experiments}
% \begin{figure}
%     \centering
%     \includegraphics[width=\textwidth]{01_cond_pdf_cdf.pdf}
%     \caption{Approximation a PDF of the total flux through a square domain with random conductivity. Use 11 Legendere moments.}
%     \label{fig:flux_approx_pdf}
% \end{figure}
% \todo{Divne je, ze je ten histogram (a stejne tak vsechny histogramy prvnich urovni) uriznuty vpravo.}
% \FloatBarrier

\subsection{Porous media flow problem}
The functionality of our approach is numerically verified on the task of groundwater flow through porous media. The purpose is to replicate the results from R. Blaheta et al. \cite{Blaheta20160413}.

\subsection{Model problem}\label{model_problem}
\todo{Should be described more precisely?}

The groundwater flow is described by the boundary value problem on the unit square:
\begin{align}
-div(K(x)\nabla p) &= 0 \\ \nonumber
-K \nabla p  \cdot \vec{n} &= 0\text{,}
\end{align}
$K$ is hydraulic conductivity, $p$ is pore water pressure, $-K \nabla p$ is Darcys velocity, $\vec{n}$ is a unit normal vector.

We are interested in the total flow $Y$ through the specified area
\begin{equation}
Y = \int_{0}^{1}[-K\nabla p \cdot \vec{n}] (1, y) dy\text{.}
\end{equation}

\begin{figure}[!htp]
\centering
\includegraphics[width=0.5\linewidth]{porous_media_flow/problem_illustration.png}
\caption{Problem illustration}
\label{fig:benchmark_task}
\centering
\end{figure}
\FloatBarrier

\subsection{Methods and parameters}
The experiments are executed under the following conditions.
The input random fields have parameters comparable to those used by R. Blaheta et. al. \cite{Blaheta20160413}. The size of the finest mesh is about 80 000 elements (87794~exactly). 

Six different spatial random fields (SRF) are in use. The GSTools software library \cite{GSTools} is utilised for generating SRF. The randomisation method, described in detail by Hesse et. al., \cite{Hesse2014}, is the core of the library. The exponential covariance model is employed for our experiments. The model is given by the following correlation function:
$$
\rho(r) = \exp\left(-\frac{r}{\lambda}\right),
$$
where $\lambda$ is a correlation length. The generation of random fields by the randomisation method can be expressed as follows (for details see  \cite{Hesse2014}[p. 35])
$$
u(x) = \sqrt{2 \frac{\sigma^2}{N}} \sum_{i=1}^{N}{\left( Z_i^1\cos(2\pi k_i x) + Z_i^2\sin(2\pi k_i x)   \right)},
$$
where $Z_i^j$ are independent Gaussian random variables, $k_i$ are samples from the spectral density distribution of the covariance model, $N$ is a Fourier mode number.
Our experiments show that for our problem \ref{model_problem}, it is advisable to set $N=10^4$ instead of the default $N=10^3$. This adjustment brings a slight increase in computational cost. On the other hand, it significantly reduces "artefacts" in random fields that might occur with smaller $N$. %http://www.ib.pwr.wroc.pl/wpula/w14.pdf
Two correlation lengths $\lambda \in \{0.1, 0.3\}$ are considered, and three distinct standard deviations $\sigma \in \{1, 2, 4\}$ are compared for both.

Standard Monte Carlo method and multilevel Monte Carlo methods are executed. In both cases, the number of samples $\vc N$ is determined with respect to attaining the target variance $V$ of each of $M$ moments. In this case, $V=10^{-5}$ and $M=25$. In each of the six cases, the moments are calculated on the same domain, so it is possible to compare chosen Monte Carlo methods.

The trust-region numerical optimization method is employed to solve the maximum entropy method numerically. %described in section \ref{mem}.
Python SciPy library implementation of that method is adopted (for details see  \cite{Conn20000125}[p. 169 - 200]). The tolerance of the method is set to $10^{-7}$. It means a gradient norm must be less than $10^{-7}$ to terminate successfully. The maximum number of iterations is 30, which is sufficient for all our experiments. Initial $\vl = [1, 0, ..., 0]$. 



% \subsubsection{Spatial random fields generator}
% One of the crucial things to keep in mind when using Monte Carlo methods is a proper generation of random data.
% In our case, the GSTools software library \cite{GSTools} was utilized for generating spatial random fields (SRF).
% The core of the library uses the so-called randomization method, described in detail by Hesse et. al. \cite{Hesse2014}.

% To have comparable results with R. Blaheta et. al. we employed the exponential covariance model for our experiments. The model is given by the following correlation function:
% $$
% \rho(r) = \exp\left(-\frac{r}{\lambda}\right),
% $$
% where $\lambda$ is a correlation length.


% The generation of random fields by the randomization method can be expressed as follows (for details see \cite{Hesse2014}[p. 35])

% $$
% u(x) = \sqrt{2 \frac{\sigma^2}{N}} \sum_{i=1}^{N}{\left( Z_i^1\cos(2\pi k_i x) + Z_i^2\sin(2\pi k_i x)   \right)},
% $$
% where $Z_i^j$ are independent Gaussian random variables, $k_i$ are samples from the spectral density distribution of the covariance model, $N$ is Fourier mode number.


% Our experiments show that for our problem \ref{model_problem} it is advisable to set $N = \num{1e-4}$ instead of the default $N = \num{1e-3}$. This change brings slight increase in computational cost. On the other hand, it significantly reduces "artifacts" in random fields that occurred for smaller $N$. %http://www.ib.pwr.wroc.pl/wpula/w14.pdf
 


MLMC library \cite{mlmclib} and Flow123d simulator \cite{flow123d} are utilized to schedule samples and perform underground water flow simulation. 

% \begin{figure}[!htp]
%     \centering 
% \begin{subfigure}{0.5\textwidth}
%   \includegraphics[width=\linewidth]{porous_media_flow/cl_0_1_s_1_m_25.pdf}
%   \caption{$\sigma = 1, \lambda=0.1$}
%   \label{fig:cl_0_1_s_1}
% \end{subfigure}\hfil 
% \begin{subfigure}{0.5\textwidth}
%   \includegraphics[width=\linewidth]{porous_media_flow/cl_0_3_s_1_m_25.pdf}
%   \caption{$\sigma = 1, \lambda=0.3$}
%   \label{fig:cl_0_3_s_1}
% \end{subfigure}\hfil 

% \centering
% \begin{subfigure}{0.5\textwidth}
%   \includegraphics[width=\linewidth]{porous_media_flow/cl_0_1_s_2_m_25.pdf}
%   \caption{$\sigma = 2, \lambda=0.1$}
%   \label{fig:cl_0_1_s_2}
% \end{subfigure}\hfil
% \centering
% \begin{subfigure}{0.5\textwidth}
%   \includegraphics[width=\linewidth]{porous_media_flow/cl_0_3_s_2_m_25.pdf}
%   \caption{$\sigma = 2, \lambda=0.3$}
%   \label{fig:cl_0_3_s_2}
% \end{subfigure}\hfil 

% \ContinuedFloat
% \begin{subfigure}{0.5\textwidth}
%   \includegraphics[width=\linewidth]{porous_media_flow/cl_0_1_s_4_m_25.pdf}
%   \caption{$\sigma = 4, \lambda=0.1$}
%   \label{fig:cl_0_1_s_4}
% \end{subfigure}\hfil
% \begin{subfigure}{0.5\textwidth}
%   \includegraphics[width=\linewidth]{porous_media_flow/cl_0_3_s_4_m_25.pdf}
%   \caption{$\sigma = 4, \lambda=0.3$}
%   \label{fig:cl_0_3_s_4}
% \end{subfigure}
% \caption{Reconstructed PDFs and CDFs for executed experiments}
% \label{fig:flow_distrs}
% \end{figure}
% \FloatBarrier



% \begin{figure}[!htp]
%     \centering 
% \begin{subfigure}{0.5\textwidth}
%   \includegraphics[width=\linewidth]{porous_media_flow/MLMC/cl_0_1_s_1_m_25.pdf}
%   \caption{$\sigma = 1, \lambda=0.1$}
%   \label{fig:cl_0_1_s_1}
% \end{subfigure}\hfil 
% \begin{subfigure}{0.5\textwidth}
%   \includegraphics[width=\linewidth]{porous_media_flow/MLMC/cl_0_3_s_1_m_25.pdf}
%   \caption{$\sigma = 1, \lambda=0.3$}
%   \label{fig:cl_0_3_s_1}
% \end{subfigure}\hfil 

% \centering
% \begin{subfigure}{0.5\textwidth}
%   \includegraphics[width=\linewidth]{porous_media_flow/MLMC/cl_0_1_s_2_m_25.pdf}
%   \caption{$\sigma = 2, \lambda=0.1$}
%   \label{fig:cl_0_1_s_2}
% \end{subfigure}\hfil
% \centering
% \begin{subfigure}{0.5\textwidth}
%   \includegraphics[width=\linewidth]{porous_media_flow/MLMC/cl_0_3_s_2_m_25.pdf}
%   \caption{$\sigma = 2, \lambda=0.3$}
%   \label{fig:cl_0_3_s_2}
% \end{subfigure}\hfil 

% \ContinuedFloat
% \begin{subfigure}{0.5\textwidth}
%   \includegraphics[width=\linewidth]{porous_media_flow/MLMC/cl_0_1_s_4_m_25.pdf}
%   \caption{$\sigma = 4, \lambda=0.1$}
%   \label{fig:cl_0_1_s_4}
% \end{subfigure}\hfil
% \begin{subfigure}{0.5\textwidth}
%   \includegraphics[width=\linewidth]{porous_media_flow/MLMC/cl_0_3_s_4_m_25.pdf}
%   \caption{$\sigma = 4, \lambda=0.3$}
%   \label{fig:cl_0_3_s_4}
% \end{subfigure}
% \caption{Reconstructed PDFs for executed MLMC experiments. MLMC methods in a particular subgraph have the same domain. For that reason, #L1 results are not the same as in the picture \ref{fig:flow_distrs}.}
% \label{fig:mlmc_flow_distrs}
% \end{figure}
% \FloatBarrier

\subsection{Results}
The results of the performed experiments are depicted in the Figure \ref{fig:flow_distrs}. The Standard Monte Carlo method with $10^5$ samples is considered to provide a sufficiently accurate solution. Thus its reconstructed PDF $\hat{\rho}_{ref}$ is denoted as reference (ref), and it is drawn alongside with histogram. Other reconstructed PDF's $\hat{\rho}_{L}$  originate from data obtained by Monte Carlo methods with a different number of levels $L = 1, 2, 3, 5$ under the constraint of attaining target variance. 
KL divergence $D$ is utilized to compare densities with MC density, $D := D(\hat{\rho}_{ref}|\hat{\rho}_{L})$.
Given that the method \ref{nonlinear_solver} dropping some moments from the original number of moments, the letter $M$ in the picture indicates the number of moments actually used out of the original $25$ moments.
\begin{figure}[!htp]
    \centering 
\begin{subfigure}{0.5\textwidth}
  \includegraphics[width=\linewidth]{porous_media_flow/MLMC/cl_0_1_s_1.pdf}
  \caption{$\sigma = 1, \lambda=0.1$}
  \label{fig:cl_0_1_s_1}
\end{subfigure}\hfil 
\begin{subfigure}{0.5\textwidth}
  \includegraphics[width=\linewidth]{porous_media_flow/MLMC/cl_0_3_s_1.pdf}
  \caption{$\sigma = 1, \lambda=0.3$}
  \label{fig:cl_0_3_s_1}
\end{subfigure}\hfil 

\centering
\begin{subfigure}{0.5\textwidth}
  \includegraphics[width=\linewidth]{porous_media_flow/MLMC/cl_0_1_s_2.pdf}
  \caption{$\sigma = 2, \lambda=0.1$}
  \label{fig:cl_0_1_s_2}
\end{subfigure}\hfil
\centering
\begin{subfigure}{0.5\textwidth}
  \includegraphics[width=\linewidth]{porous_media_flow/MLMC/cl_0_3_s_2.pdf}
  \caption{$\sigma = 2, \lambda=0.3$}
  \label{fig:cl_0_3_s_2}
\end{subfigure}\hfil 

\ContinuedFloat
\begin{subfigure}{0.5\textwidth}
  \includegraphics[width=\linewidth]{porous_media_flow/MLMC/cl_0_1_s_4.pdf}
  \caption{$\sigma = 4, \lambda=0.1$}
  \label{fig:cl_0_1_s_4}
\end{subfigure}\hfil
\begin{subfigure}{0.5\textwidth}
  \includegraphics[width=\linewidth]{porous_media_flow/MLMC/cl_0_3_s_4.pdf}
  \caption{$\sigma = 4, \lambda=0.3$}
  \label{fig:cl_0_3_s_4}
\end{subfigure}
\caption{Reconstructed PDFs. Term 'ref' denotes reference PDF $\hat{\rho}_{ref}$, PDFs $\hat{\rho}_{L}$ based on Monte Carlo methods with $L$ levels are also drawn.  $D:=D(\hat{\rho}_{ref}|\hat{\rho}_{L})$. $M$ is number of moments actually used.}
\label{fig:flow_distrs}
\end{figure}
\FloatBarrier

In comparison with R. Blaheta et. al. \cite[Figure 8]{Blaheta20160413} stated approach combining MLMC and maximum entropy method is able to provide us with smoother PDFs.


In some cases, most seen especially for the cases with $\sigma=1$. The ripples occur in the course of PDF. This effect tends to accentuate with increasing error in moment estimates. To suppress the ripples, suitable regularization is employed. We shall describe this approach in the next article.


As can be seen in the figures \ref{fig:cl_0_1_s_4} and \ref{fig:cl_0_3_s_4} it might be difficult to capture the peak of substantially skew distributions. To improve that, PDF reconstruction from the logarithm of the data is employed. Then it is possible to depict the course of distribution as seen in the figure \ref{fig:log_flow_distrs}.

\begin{figure}[!htp]
    \centering 
\begin{subfigure}{0.5\textwidth}
  \includegraphics[width=\linewidth]{porous_media_flow/MLMC/log/cl_0_1_s_4.pdf}
  \caption{$\sigma = 4, \lambda=0.1$}
  \label{fig:cl_0_1_s_4_log}
\end{subfigure}\hfil 
\begin{subfigure}{0.5\textwidth}
  \includegraphics[width=\linewidth]{porous_media_flow/MLMC/log/cl_0_3_s_4.pdf}
  \caption{$\sigma = 4, \lambda=0.3$}
  \label{fig:cl_0_3_s_4_log}
\end{subfigure}\hfil 
\caption{Reconstructed PDFs based on logarithm of data}
\label{fig:log_flow_distrs}
\end{figure}
\FloatBarrier


The following tables provide a detailed insight into the executed MLMCs which allows reproducing our results. The number of mesh elements at each level, stated in the table \ref{tab:mesh_elements}, is determined according to the prescribed mesh step range $[h_1, h_L]$, in our case: $[1, 0.0055]$. The mesh step for the particular level is calculated as follows: $h_l = h_1^{(1 - \frac{l-1}{L-1})} h_L^{ \frac{l-1}{L-1}}$.

\begin{table}[h]
    \centering
    \begin{tabular}{lccccc}
        \toprule
        \multirow{2}{*}{$L$} & \multicolumn{5}{c}{number of mesh elements} \\
        \cmidrule(lr){2-6}  \\
        {} & $L_1$ & $L_2$  & $L_3$ & $L_4$ &$L_5$ \\
        \midrule
        1 & 87794 &  &   &   &                	 \\
        2 & 6 & 87794   &    &  &             	 \\
        3 & 6 & 546 & 87794  &   &        		  \\
        5 & 6 & 48 & 546 &  6772 &  87794        \\
        
        \bottomrule
    \end{tabular}
    \caption{Number of mesh elements at levels}
       \label{tab:mesh_elements}
\end{table}

The table \ref{tab:n_samples} shows the total amount of samples at each level. Data is provided for each of the six different random field configurations. The corresponding $N_l$ increases with increasing $\sigma$ of a random field, where it is more difficult to attain the target variance $V$.

The computational costs of Monte Carlo methods are completed in the table~\ref{tab:costs}. The most computationally effective methods of the executed methods is the five-level Monte Carlo. This applies to all listed random field configurations.  


\begin{table}[h]
    \centering
    \begin{tabular}{@{} l
                @{\hspace*{2mm}}     c
                @{\hspace*{\lengtha}}c
                @{\hspace*{\lengtha}}c
                @{\hspace*{\lengtha}}c
                @{\hspace*{\lengtha}}c
                @{\hspace*{\lengtha}}c
                @{\hspace*{\lengtha}}c
                @{\hspace*{\lengtha}}c
                @{\hspace*{\lengtha}}c
                @{\hspace*{\lengtha}}c
                @{\hspace*{\lengtha}}c
                @{\hspace*{\lengtha}}c @{}}
        \toprule
        \multirow{2}{*}{$L$} & \multicolumn{5}{c}{$\lambda = 0.1$} & \multicolumn{5}{c}{$\lambda = 0.3$}\\
        \cmidrule(lr){2-6} \cmidrule(lr){7-11} \\
        {} & $N_1$ & $N_2$  & $N_3$ & $N_4$ &$N_5$ &  $N_1$ & $N_2$  & $N_3$ & $N_4$ & $N_5$ \\
        \midrule
        $\mathit{\sigma}=1$ \\
        1 & 16448 &  &  &  &               	     & 19996 &  &  &  &   \\
        2 & 35210 & 16634 &  &  &            	     & 38669 & 22717 &  &  &  \\
        3 & 62641 & 47559 & 3497 &  &       		 & 43622 & 23811 & 619 &  &    \\
        5 & 76818 & 62213 & 25241 & 7186 & 1144     & 51198 & 39906 & 21403 & 1799 & 140   \\
        
         \midrule
        $\mathit{\sigma}=2$ \\
        1 & 16819 &  &  &  &              	     & 19998 &  &  &  &   \\
        2 & 41145 & 23684 &  &  &             	     & 39673 & 24012 &  &  &   \\
        3 & 41749 & 41415 & 10300 &  &       		 & 58239 & 49400 & 3032 &  &   \\
        5 & 91202 & 73695 & 35857 & 10582 & 1546    & 67150 & 60696 & 19636 & 3544 & 499    \\
        
         \midrule
        $\mathit{\sigma}=4$ \\
        1 & 19449 &  &  &  &                	     &  22322 &  &  &  &   \\
        2 & 46852 & 32145 &  &  &             	     & 47128 & 24731 &  &  & \\
        3 & 69732 & 92373 & 11996 &  &      		 &  70652 & 72607 & 5036 &  &   \\
        5 & 117294 & 174254 & 84036 & 31364 & 4854      & 113190 & 86391 & 34113 & 6844 & 876    \\
        \bottomrule
    \end{tabular}
    \caption{Number of samples at Monte Carlo method levels}
    \label{tab:n_samples}
\end{table}
\FloatBarrier


\begin{table}[h]
    \centering
    \begin{tabular}{@{} l
                @{\hspace*{1.5mm}}     c
                @{\hspace*{\lengthb}}c
                @{\hspace*{\lengthb}}c
                @{\hspace*{\lengthb}}c
                @{\hspace*{\lengthb}}c
                @{\hspace*{\lengthb}}c|
                @{\hspace*{\lengthb}}c
                @{\hspace*{\lengthb}}c
                @{\hspace*{\lengthb}}c
                @{\hspace*{\lengthb}}c
                @{\hspace*{\lengthb}}c
                @{\hspace*{\lengthb}}c
                @{\hspace*{\lengthb}}c
                @{\hspace*{\lengthb}}c @{}}
        \toprule
        \multirow{2}{*}{$L$} & \multicolumn{6}{c}{$\lambda = 0.1$} & \multicolumn{6}{c}{$\lambda = 0.3$}\\
        \cmidrule(lr){2-7} \cmidrule(lr){8-13} \\
        {} & $C_1 N_1$ & $C_2 N_2$  & $C_3 N_3$ & $C_4 N_4$ &$C_5 N_5$ & $C$ & $C_1 N_1$ & $C_2 N_2$  & $C_3 N_3$ & $C_4 N_4$ &$C_5 N_5$ & $C$ &\\
        \midrule
        $\mathit{\sigma}=1$ \\
        1 &918932 &  &  &  &  & 918932              	     &  1133537 &  &  &  &  & 1133537    \\
        2 & 12537 & 937408 &  &  &  & 949945             	     & 14569 & 1339607 &  &  &  & 1354176  \\
        3 & 20090 & 44094 & 200505 &  &  & 264689       		 & 15428 & 21891 & 36077 &  &  & 73396\\
        5 & 24653 & 39048 & 23916 & 39099 & 69471 & 196188     & 17466 & 24546 & 20281 & 9807 & 8815 & 80914    \\
        
         \midrule
        $\mathit{\sigma}=2$ \\
        1 & 988565 &  &  &  &  & 988565            	     & 1142812 &  &  &  &  & 1142812    \\
        2 & 13687 & 1365048 &  &  &  & 1378735             	     & 15165 & 1392662 &  &  &  & 1407827  \\
        3 & 17422 & 38260 & 591138 &  &  & 646820         		 & 19928 & 45796 & 171585 &  &  & 237309  \\
        5 & 30210 & 46933 & 35236 & 58457 & 94330 & 265167    & 25986 & 42131 & 20333 & 18864 & 31417 & 138731     \\
        
         \midrule
        $\mathit{\sigma}=4$ \\
        1 & 1215201 &  &  &  &  & 1215201              	     &  1294468 &  &  &  &  & 1294468   \\
        2 &  15958 & 1987856 &  &  &  & 2003815            	     &  16527 & 1443551 &  &  &  & 1460079 \\
        3 & 23805 & 86776 & 681878 &  &  & 792459     		 & 28644 & 67778 & 293154 &  &  & 389576    \\
        5 & 48777 & 111978 & 80789 & 166682 & 316138 & 724365      & 42926 & 53610 & 31584 & 33490 & 54784 & 216393    \\
        \bottomrule
    \end{tabular}
    \caption{Monte Carlo methods computational costs $C_i N_i$ (s) at levels and total cost $C$ (s)}
    \label{tab:costs}
\end{table}
\FloatBarrier


\subsection{MLMC sampling algorithm}
Optimal choice of the sample vector $\vc N$ given by \eqref{eq:opt_n_for_var} or \eqref{eq:opt_n_for_cost} is function $N^{opt}(\vc V, \vc C)$ of the vector of level variances $\vc V$ and the vector of level costs $\vc C$. As $\vc V$ and $\vc C$ are not known a priori, we intend to estimate them on the fly from the increasing set of collected samples. Level variances $V_l$ are estimated by the regression described in the section \ref{est_var_cost}, sample cost $C_l$ is simply the average sample time of collected samples on the level $l$. 
In order to collect an optimal number of samples, we designed a simple iterative Algorithm \ref{algo_sampling}. 
The initial vector of scheduled samples $N^s_0$ is fixed. In particular, we use a geometric sequence decreasing from $100$ to $0$. Vector $\vc N^t_0$ of the target number of samples is set to $2\,\vc N^s_0$. The iteration $i$ starts with scheduling new samples up to the $\vc N^s_i$.
Then we wait until at least half of $\vc N^s_i$ is computed. We collect computed samples and utilize all available samples to update estimates $\vc V_i$ and $\vc C_i$ for level variances and costs, respectively.
Employing these new estimates, we determine the new target sampling vector $\vc N^t_{i+1}$.
The fraction $\alpha$ of the difference between $\vc N^t_{i+1}$ and $\vc N^t_{i}$ determines the number of added scheduled samples. We iterate until the relative difference between target and scheduled sample vectors is greater than tolerance $\epsilon$ for each level $l$.
In practice we use parameters $\alpha=0.1$ and $\epsilon = 0.5$.

\begin{algorithm}
    \SetAlgoLined
    \DontPrintSemicolon
    %
    $\vc N^s_0 \gets ( 100, ..., 3)$\;
    $\vc N^t_0 = 2 \vc N^s_0$\;
    $i \gets 0$\;
    \While{ $N^t_{i,l} - N^s_{i,l} > \eps N^s_{i,l}$ for each $l$ }{
        schedule new samples up to $\vc N^s_i$\;
        wait until half of samples is done\;
        estimate $\vc V_i$, $\vc C_i$ from collected samples\;
        $\vc N^t_{i+1} \gets N^{opt}(\vc V_i, \vc C_i)$\;
        $\vc N^s_{i+1} \gets \vc N^s_i + \alpha(\vc N^t_{i+1} - \vc N^s_i)$\;
        $i \gets i+1$\;
    }
    \caption{MLMC sampling}
    \label{algo_sampling}
\end{algorithm}

The Figure \ref{fig:n_l_time} illustrates the progress of the algorithm. For each iteration of the proposed algorithm, the total CPU times and CPU simulation times are depicted. The five-level Monte Carlo method is performed for simulations of the described porous media flow problem. The simulations are run by the flow123d software \cite{flow123d}. Random field generation, inclusive of creating flow123d input mesh files, is also included in the simulation's CPU time. The difference between the total and the simulation time represents the overhead costs of our algorithm. 

Different meshes were created at each level. First level mesh has just 6 elements; higher levels meshes consist of 48, 546, 6772 and 87794 elements. The whole task was performed on the cluster that contains 20 nodes, each of which has 2x 10-core Intel Xeon Silver 4114 CPU (2.2GHz) and 2x 8 GB DDR4 2400 ECC Reg dual rank. This allows us to perform parallel computing of samples. Individual sample computations were grouped to the jobs that were scheduled by PBS software. Each job had approximately the same computational cost. Cluster traffic affects the number of currently running jobs. As a result, this may affect the continuity of sample collection. That is the reason why leaps occur in $C^c$.

In general, the main decrease of $C^t$ occurs during the first few iterations. Despite the course of $C^t$, the successive iterations might also be beneficial. Concerning the negligible overheads of the sampling algorithm, we can afford to run other iterations within which samples are redistributed across levels. It is beneficial if there are small differences in variances between some levels. At the end of the sampling algorithm, the remaining scheduled samples are collected when finished.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{sampling_log.pdf}
\caption{Evolution of algorithm \ref{algo_sampling} total and simulation (sim) CPU time. $C^t_{i}$ for target samples $N^t_{i}$, $C^s_{i}$ for scheduled samples $N^s_{i}$, $C^c_{i}$ for collected samples $N^t_{i} - N^s_{i}$.}
\label{fig:n_l_time}
\end{figure}


% \begin{table}[]
% \caption{Sampling algorithm iterations}
% \centering

% \begin{tabular}{|l|l|l|l|l|l|l|}
% \hline
% iteration                                  & samples  & $l_{1}$     & $l_{2}$   & $l_{3}$  & $l_{4}$ & $l_{5}$ \\ \hhline{|=|=|=|=|=|=|=|}
% {\multirow{2}{*}{0}} & collected & 100    & 41    & 17  & 7  & 2  \\ 
%                   & estimated & 2204   & 402   & 17  & 7  & 3  \\ \hline
% \multirow{2}{*}{1}                       & collected & 1112   & 402   & 17  & 7  & 3  \\  
%                                          & estimated & 3955   & 612   & 17  & 7  & 3  \\ \hline
% \multirow{2}{*}{2}                       & collected & 2103   & 432   & 17  & 7  & 3  \\  
%                                          & estimated & 5545   & 801   & 17  & 7  & 3  \\ \hline
% \multirow{2}{*}{4}                       & collected & 3777   & 654   & 17  & 7  & 3  \\  
%                                          & estimated & 8261   & 1118  & 17  & 7  & 3  \\ \hline
% \multirow{2}{*}{10}                      & collected & 6517   & 926   & 17  & 7  & 3  \\  
%                                          & estimated & 12979  & 1642  & 17  & 7  & 3  \\ \hline
% \multirow{2}{*}{20}                      & collected & 9457   & 1268  & 17  & 7  & 3  \\  
%                                          & estimated & 17181  & 2078  & 17  & 7  & 3  \\ \hline
% \multirow{2}{*}{30}                      & collected & 9972   & 1293  & 17  & 7  & 3  \\  
%                                          & estimated & 18671  & 2247  & 17  & 7  & 3  \\ \hline
% \multirow{2}{*}{55}                      & collected & 16782  & 2051  & 17  & 7  & 3  \\  
%                                          & estimated & 19544  & 2370  & 17  & 7  & 3  \\ \hline
% \multirow{2}{*}{56}                      & collected & 16783  & 2051  & 17  & 7 & 3  \\  
%                                          & estimated & 19556  & 2373  & 17  & 7 & 3  \\ \hline
% \end{tabular}
% \end{table}
% \FloatBarrier





\section{Conclusions}



\bibliographystyle{plain}
\bibliography{theory.bib}








\end{document}

